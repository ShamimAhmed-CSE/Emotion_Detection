{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db863407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:44:39.043441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 11:44:39.254965: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-24 11:44:39.254983: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-24 11:44:40.416935: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-24 11:44:40.417048: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-24 11:44:40.417078: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a8b2300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'train'\n",
    "val_dir = 'test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1013a2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:44:49.158178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:44:49.158254: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-24 11:44:49.158349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-06-24 11:44:49.159406: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),padding='valid', activation='relu', input_shape=(48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99199fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 46, 46, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 21, 21, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,198,919\n",
      "Trainable params: 2,198,471\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d09d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "/tmp/ipykernel_7928/1454336883.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_info = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "448/448 [==============================] - 108s 238ms/step - loss: 1.8718 - accuracy: 0.3377 - val_loss: 1.6604 - val_accuracy: 0.3691\n",
      "Epoch 2/10\n",
      "448/448 [==============================] - 102s 228ms/step - loss: 1.4603 - accuracy: 0.4445 - val_loss: 1.3828 - val_accuracy: 0.4739\n",
      "Epoch 3/10\n",
      "448/448 [==============================] - 105s 234ms/step - loss: 1.3306 - accuracy: 0.4966 - val_loss: 1.3220 - val_accuracy: 0.5003\n",
      "Epoch 4/10\n",
      "448/448 [==============================] - 103s 229ms/step - loss: 1.2204 - accuracy: 0.5412 - val_loss: 1.2910 - val_accuracy: 0.5110\n",
      "Epoch 5/10\n",
      "448/448 [==============================] - 114s 254ms/step - loss: 1.1215 - accuracy: 0.5857 - val_loss: 1.2629 - val_accuracy: 0.5215\n",
      "Epoch 6/10\n",
      "448/448 [==============================] - 105s 234ms/step - loss: 1.0184 - accuracy: 0.6272 - val_loss: 1.2432 - val_accuracy: 0.5328\n",
      "Epoch 7/10\n",
      "448/448 [==============================] - 110s 246ms/step - loss: 0.9240 - accuracy: 0.6650 - val_loss: 1.2189 - val_accuracy: 0.5423\n",
      "Epoch 8/10\n",
      "448/448 [==============================] - 101s 225ms/step - loss: 0.8221 - accuracy: 0.7067 - val_loss: 1.2291 - val_accuracy: 0.5490\n",
      "Epoch 9/10\n",
      "448/448 [==============================] - 101s 226ms/step - loss: 0.7304 - accuracy: 0.7434 - val_loss: 1.2212 - val_accuracy: 0.5578\n",
      "Epoch 10/10\n",
      "415/448 [==========================>...] - ETA: 6s - loss: 0.6341 - accuracy: 0.7810"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "model_info = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bb3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('model73.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be517fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 11:01:06.248937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 11:01:06.449664: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:06.449685: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-24 11:01:07.416073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:07.416196: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:07.416211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-06-24 11:01:08.531158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 11:01:08.531192: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-24 11:01:08.531217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-06-24 11:01:08.531698: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#load trained model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "emotion_model = load_model(\"model73.h5\")\n",
    "\n",
    "# prevents openCL usage and unnecessary logging messages\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# dictionary mapping class labels with corresponding emotions\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "# start the webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break;\n",
    "        \n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    facecasc = cv2.CascadeClassifier('/home/shamim/Downloads/haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('Video', cv2.resize(frame,(500,500),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412b04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 14:38:53.700343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-24 14:38:54.030488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-24 14:38:54.030549: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-24 14:38:56.490768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-24 14:38:56.490920: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-24 14:38:56.490932: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for emotion detaction and 2 for upscaling1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 14:39:03.445469: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shamim/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-06-24 14:39:03.445499: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-24 14:39:03.445534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rakib): /proc/driver/nvidia/version does not exist\n",
      "2023-06-24 14:39:03.445933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 467ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "n =int(input(\"Enter 1 for emotion detaction and 2 for upscaling\"))\n",
    "\n",
    "if n == 1:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from keras.models import load_model\n",
    "\n",
    "    # Load trained model\n",
    "    emotion_model = load_model(\"model73.h5\")\n",
    "\n",
    "    # Dictionary mapping class labels with corresponding emotions\n",
    "    emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread('nr.jpeg')\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find haar cascade to draw bounding box around face\n",
    "    face_cascade = cv2.CascadeClassifier('/home/shamim/Downloads/haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(image, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        max_index = int(np.argmax(prediction))\n",
    "        emotion_label = emotion_dict[max_index]\n",
    "        cv2.putText(image, emotion_label, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the image\n",
    "    cv2.imshow('Emotion Detection', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "elif n==2:\n",
    "    img = cv2.imread('camel.jpg')\n",
    " \n",
    "    image_plot = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.title(image_plot.shape)\n",
    "    plt.imshow(image_plot)\n",
    "    plt.show()\n",
    " \n",
    "    # Running the SR model\n",
    " \n",
    "    # Model to preprocess the images\n",
    " \n",
    " \n",
    "    def preprocessing(img):\n",
    "        imageSize = (tf.convert_to_tensor(image_plot.shape[:-1]) // 4) * 4\n",
    "        cropped_image = tf.image.crop_to_bounding_box(\n",
    "            img, 0, 0, imageSize[0], imageSize[1])\n",
    "        preprocessed_image = tf.cast(cropped_image, tf.float32)\n",
    "        return tf.expand_dims(preprocessed_image, 0)\n",
    " \n",
    " \n",
    "    # This is a model of Enhanced Super Resolution GAN Model\n",
    "    # The link given here is a model of ESRGAN model\n",
    "    esrgn_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\n",
    "    model = hub.load(esrgn_path)\n",
    " \n",
    "    # Model to employ the model\n",
    " \n",
    " \n",
    "    def srmodel(img):\n",
    "        preprocessed_image = preprocessing(img)  # Preprocess the LR Image\n",
    "        new_image = model(preprocessed_image)  # Runs the model\n",
    "        # returns the size of the original argument that is given as input\n",
    "        return tf.squeeze(new_image) / 255.0\n",
    " \n",
    " \n",
    "    # Plot the HR image\n",
    "    hr_image = srmodel(image_plot)\n",
    "    plt.title(hr_image.shape)\n",
    "    plt.imshow(hr_image)\n",
    "    plt.show()\n",
    "\n",
    "else: \n",
    "    exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb602e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
